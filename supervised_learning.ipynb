{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPdlHZi4pKKVVE55UwOmG/P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BuweiChen/mini-alpha-go-for-chess/blob/main/supervised_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWksMLrUllRo",
        "outputId": "4eaef685-d5c6-4442-cf1d-65b2547d50aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Dataset URL: https://www.kaggle.com/datasets/arevel/chess-games\n",
            "License(s): CC0-1.0\n",
            "Downloading chess-games.zip to /content\n",
            " 99% 1.44G/1.45G [00:22<00:00, 54.8MB/s]\n",
            "100% 1.45G/1.45G [00:22<00:00, 68.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "! pip install kaggle -q\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! kaggle datasets download arevel/chess-games\n",
        "! unzip -qq /content/chess-games.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install chess -q"
      ],
      "metadata": {
        "id": "k7UAUQWrqptY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "letter_to_num = {'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7}\n",
        "num_to_letter = {0: 'a', 1: 'b', 2: 'c', 3: 'd', 4: 'e', 5: 'f', 6: 'g', 7: 'h'}"
      ],
      "metadata": {
        "id": "VDF4_Ccuma6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import chess\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "import gc"
      ],
      "metadata": {
        "id": "-f9bU6f0q4dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def board_to_rep(board):\n",
        "  pieces = ['p', 'r', 'n', 'k', 'q', 'b']\n",
        "  layers = []\n",
        "  for piece in pieces:\n",
        "    layers.append(create_rep_layer(board, piece))\n",
        "  board_rep = np.stack(layers)\n",
        "  return board_rep"
      ],
      "metadata": {
        "id": "mTn0_1WXqLrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_rep_layer(board, type):\n",
        "  s = str(board)\n",
        "  s = re.sub(f'[^{type}{type.upper()} \\n]', '.', s)\n",
        "  s = re.sub(f'{type}', '-1', s)\n",
        "  s = re.sub(f'{type.upper()}', '1', s)\n",
        "  s = re.sub(f'\\.', '0', s)\n",
        "  board_mat = []\n",
        "  for row in s.split('\\n'):\n",
        "    row = row.split(' ')\n",
        "    row = [int(x) for x in row]\n",
        "    board_mat.append(row)\n",
        "\n",
        "  return np.array(board_mat)"
      ],
      "metadata": {
        "id": "eokwxK57rhB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_board = chess.Board(\"r1bqkb1r/pppp1Qpp/2n2n2/4p3/2B1P3/8/PPPP1PPP/RNB1K1NR b KQkq - 0 4\")\n",
        "print(test_board)\n",
        "print(board_to_rep(test_board))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-mye6KDvxh2",
        "outputId": "33dc36bb-525e-47e8-a049-e1c675d0ce94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "r . b q k b . r\n",
            "p p p p . Q p p\n",
            ". . n . . n . .\n",
            ". . . . p . . .\n",
            ". . B . P . . .\n",
            ". . . . . . . .\n",
            "P P P P . P P P\n",
            "R N B . K . N R\n",
            "[[[ 0  0  0  0  0  0  0  0]\n",
            "  [-1 -1 -1 -1  0  0 -1 -1]\n",
            "  [ 0  0  0  0  0  0  0  0]\n",
            "  [ 0  0  0  0 -1  0  0  0]\n",
            "  [ 0  0  0  0  1  0  0  0]\n",
            "  [ 0  0  0  0  0  0  0  0]\n",
            "  [ 1  1  1  1  0  1  1  1]\n",
            "  [ 0  0  0  0  0  0  0  0]]\n",
            "\n",
            " [[-1  0  0  0  0  0  0 -1]\n",
            "  [ 0  0  0  0  0  0  0  0]\n",
            "  [ 0  0  0  0  0  0  0  0]\n",
            "  [ 0  0  0  0  0  0  0  0]\n",
            "  [ 0  0  0  0  0  0  0  0]\n",
            "  [ 0  0  0  0  0  0  0  0]\n",
            "  [ 0  0  0  0  0  0  0  0]\n",
            "  [ 1  0  0  0  0  0  0  1]]\n",
            "\n",
            " [[ 0  0  0  0  0  0  0  0]\n",
            "  [ 0  0  0  0  0  0  0  0]\n",
            "  [ 0  0 -1  0  0 -1  0  0]\n",
            "  [ 0  0  0  0  0  0  0  0]\n",
            "  [ 0  0  0  0  0  0  0  0]\n",
            "  [ 0  0  0  0  0  0  0  0]\n",
            "  [ 0  0  0  0  0  0  0  0]\n",
            "  [ 0  1  0  0  0  0  1  0]]\n",
            "\n",
            " [[ 0  0  0  0 -1  0  0  0]\n",
            "  [ 0  0  0  0  0  0  0  0]\n",
            "  [ 0  0  0  0  0  0  0  0]\n",
            "  [ 0  0  0  0  0  0  0  0]\n",
            "  [ 0  0  0  0  0  0  0  0]\n",
            "  [ 0  0  0  0  0  0  0  0]\n",
            "  [ 0  0  0  0  0  0  0  0]\n",
            "  [ 0  0  0  0  1  0  0  0]]\n",
            "\n",
            " [[ 0  0  0 -1  0  0  0  0]\n",
            "  [ 0  0  0  0  0  1  0  0]\n",
            "  [ 0  0  0  0  0  0  0  0]\n",
            "  [ 0  0  0  0  0  0  0  0]\n",
            "  [ 0  0  0  0  0  0  0  0]\n",
            "  [ 0  0  0  0  0  0  0  0]\n",
            "  [ 0  0  0  0  0  0  0  0]\n",
            "  [ 0  0  0  0  0  0  0  0]]\n",
            "\n",
            " [[ 0  0 -1  0  0 -1  0  0]\n",
            "  [ 0  0  0  0  0  0  0  0]\n",
            "  [ 0  0  0  0  0  0  0  0]\n",
            "  [ 0  0  0  0  0  0  0  0]\n",
            "  [ 0  0  1  0  0  0  0  0]\n",
            "  [ 0  0  0  0  0  0  0  0]\n",
            "  [ 0  0  0  0  0  0  0  0]\n",
            "  [ 0  0  1  0  0  0  0  0]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def move_to_rep(move, board):\n",
        "  board.push_san(move).uci()\n",
        "  move = str(board.pop())\n",
        "\n",
        "  from_output_layer = np.zeros((8,8))\n",
        "  from_row = 8 - int(move[1])\n",
        "  from_column = letter_to_num[move[0]]\n",
        "  from_output_layer[from_row, from_column] = 1\n",
        "\n",
        "  to_output_layer = np.zeros((8,8))\n",
        "  to_row = 8 - int(move[3])\n",
        "  to_column = letter_to_num[move[2]]\n",
        "  to_output_layer[to_row, to_column] = 1\n",
        "\n",
        "  return np.stack([from_output_layer, to_output_layer])"
      ],
      "metadata": {
        "id": "d_nW8jTZs1Z3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_move_list(s):\n",
        "  return re.sub('\\d*\\. ', '', s).split(' ')[:-1]"
      ],
      "metadata": {
        "id": "RfGKHtPsunCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chess_data_raw = pd.read_csv('/content/chess_games.csv', usecols=['AN', 'WhiteElo'])\n",
        "chess_data = chess_data_raw[chess_data_raw['WhiteElo'] > 2000]\n",
        "del chess_data_raw\n",
        "gc.collect()\n",
        "chess_data = chess_data[['AN']]\n",
        "chess_data = chess_data[~chess_data['AN'].str.contains('{')]\n",
        "chess_data = chess_data[chess_data['AN'].str.len() > 20]\n",
        "print(chess_data.shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZbhU7XldI6R",
        "outputId": "b7a89bae-5dc8-46f3-c75d-7e643f9de7cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "883376\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ChessDataset(Dataset):\n",
        "  def __init__(self, games):\n",
        "    super(ChessDataset, self).__init__()\n",
        "    self.games = games\n",
        "\n",
        "  def __len__(self):\n",
        "    return 40000\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    game_i = np.random.randint(self.games.shape[0])\n",
        "    random_game = chess_data['AN'].values[game_i]\n",
        "    moves = create_move_list(random_game)\n",
        "    game_state_i = np.random.randint(len(moves) - 1)\n",
        "    next_move = moves[game_state_i]\n",
        "    moves = moves[:game_state_i]\n",
        "    board = chess.Board()\n",
        "    for move in moves:\n",
        "      board.push_san(move)\n",
        "    x = board_to_rep(board)\n",
        "    y = move_to_rep(next_move, board)\n",
        "    if game_state_i % 2 == 1:\n",
        "      x *= -1\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "eaJEgFHJfT37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = ChessDataset(chess_data['AN'])\n",
        "data_train_loader = DataLoader(data_train, batch_size=32, shuffle=True, drop_last=True)"
      ],
      "metadata": {
        "id": "UPZQCrquhHBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class module(nn.Module):\n",
        "  def __init__(self, hidden_size):\n",
        "    super(module, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(hidden_size, hidden_size, 3, stride=1, padding=1)\n",
        "    self.conv2 = nn.Conv2d(hidden_size, hidden_size, 3, stride=1, padding=1)\n",
        "    self.bn1 = nn.BatchNorm2d(hidden_size)\n",
        "    self.bn2 = nn.BatchNorm2d(hidden_size)\n",
        "    self.activation1 = nn.SELU()\n",
        "    self.activation2 = nn.SELU()\n",
        "  def forward(self, x):\n",
        "    x_input = torch.clone(x)\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn1(x)\n",
        "    x = self.activation1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.bn2(x)\n",
        "    x = x + x_input # residual connections\n",
        "    x = self.activation2(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "6_38nx83i00K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ChessNet(nn.Module):\n",
        "  def __init__(self, hidden_layers=4, hidden_size=200):\n",
        "    super(ChessNet, self).__init__()\n",
        "    self.hidden_layers = hidden_layers\n",
        "    self.input_layer = nn.Conv2d(6, hidden_size, 3, stride=1, padding=1)\n",
        "    self.module_list = nn.ModuleList([module(hidden_size) for i in range(hidden_layers)])\n",
        "    self.output_layer = nn.Conv2d(hidden_size, 2, 3, stride=1, padding=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.input_layer(x)\n",
        "    x = F.relu(x)\n",
        "\n",
        "    for i in range(self.hidden_layers):\n",
        "      x = self.module_list[i](x)\n",
        "\n",
        "    x = self.output_layer(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "x5yn8ZQxkdVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "  print(\"CUDA is available. Training on GPU.\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "  print(\"CUDA not available. Training on CPU.\")\n",
        "\n",
        "# Initialize the ChessNet model\n",
        "\n",
        "model_epoch = 9\n",
        "\n",
        "model = ChessNet(hidden_layers=4, hidden_size=200).to(device)\n",
        "if model_epoch >= 0:\n",
        "  model.load_state_dict(torch.load(f'chess_model_epoch_{model_epoch}.pth'))\n",
        "model.train()  # Set the model to training mode\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Define separate loss functions for different parts of the output\n",
        "metric_from = nn.CrossEntropyLoss()\n",
        "metric_to = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 100  # Number of epochs to train for\n",
        "for epoch in range(num_epochs):\n",
        "  epoch += model_epoch + 1\n",
        "  total_loss = 0.0\n",
        "  for i, (x, y) in enumerate(data_train_loader):\n",
        "    # Move data to the appropriate device (e.g., GPU or CPU)\n",
        "    x = x.float().to(device)\n",
        "    y = y.float().to(device)\n",
        "\n",
        "    # Zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    output = model(x)\n",
        "\n",
        "    # Compute loss for both outputs\n",
        "    loss_from = metric_from(output[:, 0, :], y[:, 0, :])\n",
        "    loss_to = metric_to(output[:, 1, :], y[:, 1, :])\n",
        "    loss = loss_from + loss_to\n",
        "\n",
        "    # Backward pass and optimize\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print statistics\n",
        "    total_loss += loss.item()\n",
        "    if (i + 1) % 100 == 0:  # Print every 100 mini-batches\n",
        "      print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(data_train_loader)}], Loss: {total_loss / 100:.4f}')\n",
        "      total_loss = 0.0\n",
        "\n",
        "  torch.save(model.state_dict(), f'chess_model_epoch_{epoch}.pth')\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMXPHdTal-tY",
        "outputId": "0f8c5874-4ff4-4f2c-96f2-992959ac38d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available. Training on GPU.\n",
            "Epoch [11/100], Step [100/1250], Loss: 0.1435\n",
            "Epoch [11/100], Step [200/1250], Loss: 0.1427\n",
            "Epoch [11/100], Step [300/1250], Loss: 0.1428\n",
            "Epoch [11/100], Step [400/1250], Loss: 0.1399\n",
            "Epoch [11/100], Step [500/1250], Loss: 0.1400\n",
            "Epoch [11/100], Step [600/1250], Loss: 0.1421\n",
            "Epoch [11/100], Step [700/1250], Loss: 0.1389\n",
            "Epoch [11/100], Step [800/1250], Loss: 0.1440\n",
            "Epoch [11/100], Step [900/1250], Loss: 0.1393\n",
            "Epoch [11/100], Step [1000/1250], Loss: 0.1365\n",
            "Epoch [11/100], Step [1100/1250], Loss: 0.1374\n",
            "Epoch [11/100], Step [1200/1250], Loss: 0.1410\n",
            "Epoch [12/100], Step [100/1250], Loss: 0.1407\n",
            "Epoch [12/100], Step [200/1250], Loss: 0.1409\n",
            "Epoch [12/100], Step [300/1250], Loss: 0.1389\n",
            "Epoch [12/100], Step [400/1250], Loss: 0.1413\n",
            "Epoch [12/100], Step [500/1250], Loss: 0.1369\n",
            "Epoch [12/100], Step [600/1250], Loss: 0.1381\n",
            "Epoch [12/100], Step [700/1250], Loss: 0.1368\n",
            "Epoch [12/100], Step [800/1250], Loss: 0.1347\n",
            "Epoch [12/100], Step [900/1250], Loss: 0.1394\n",
            "Epoch [12/100], Step [1000/1250], Loss: 0.1365\n",
            "Epoch [12/100], Step [1100/1250], Loss: 0.1352\n",
            "Epoch [12/100], Step [1200/1250], Loss: 0.1402\n",
            "Epoch [13/100], Step [100/1250], Loss: 0.1382\n",
            "Epoch [13/100], Step [200/1250], Loss: 0.1365\n",
            "Epoch [13/100], Step [300/1250], Loss: 0.1374\n",
            "Epoch [13/100], Step [400/1250], Loss: 0.1355\n",
            "Epoch [13/100], Step [500/1250], Loss: 0.1364\n",
            "Epoch [13/100], Step [600/1250], Loss: 0.1428\n",
            "Epoch [13/100], Step [700/1250], Loss: 0.1380\n",
            "Epoch [13/100], Step [800/1250], Loss: 0.1389\n",
            "Epoch [13/100], Step [900/1250], Loss: 0.1351\n",
            "Epoch [13/100], Step [1000/1250], Loss: 0.1342\n",
            "Epoch [13/100], Step [1100/1250], Loss: 0.1378\n",
            "Epoch [13/100], Step [1200/1250], Loss: 0.1347\n",
            "Epoch [14/100], Step [100/1250], Loss: 0.1360\n",
            "Epoch [14/100], Step [200/1250], Loss: 0.1395\n",
            "Epoch [14/100], Step [300/1250], Loss: 0.1335\n",
            "Epoch [14/100], Step [400/1250], Loss: 0.1360\n",
            "Epoch [14/100], Step [500/1250], Loss: 0.1422\n",
            "Epoch [14/100], Step [600/1250], Loss: 0.1363\n",
            "Epoch [14/100], Step [700/1250], Loss: 0.1314\n",
            "Epoch [14/100], Step [800/1250], Loss: 0.1351\n",
            "Epoch [14/100], Step [900/1250], Loss: 0.1396\n",
            "Epoch [14/100], Step [1000/1250], Loss: 0.1338\n",
            "Epoch [14/100], Step [1100/1250], Loss: 0.1339\n",
            "Epoch [14/100], Step [1200/1250], Loss: 0.1365\n",
            "Epoch [15/100], Step [100/1250], Loss: 0.1312\n",
            "Epoch [15/100], Step [200/1250], Loss: 0.1331\n",
            "Epoch [15/100], Step [300/1250], Loss: 0.1326\n",
            "Epoch [15/100], Step [400/1250], Loss: 0.1376\n",
            "Epoch [15/100], Step [500/1250], Loss: 0.1296\n",
            "Epoch [15/100], Step [600/1250], Loss: 0.1328\n",
            "Epoch [15/100], Step [700/1250], Loss: 0.1338\n",
            "Epoch [15/100], Step [800/1250], Loss: 0.1303\n",
            "Epoch [15/100], Step [900/1250], Loss: 0.1361\n",
            "Epoch [15/100], Step [1000/1250], Loss: 0.1367\n",
            "Epoch [15/100], Step [1100/1250], Loss: 0.1387\n",
            "Epoch [15/100], Step [1200/1250], Loss: 0.1359\n",
            "Epoch [16/100], Step [100/1250], Loss: 0.1344\n",
            "Epoch [16/100], Step [200/1250], Loss: 0.1343\n",
            "Epoch [16/100], Step [300/1250], Loss: 0.1343\n",
            "Epoch [16/100], Step [400/1250], Loss: 0.1326\n",
            "Epoch [16/100], Step [500/1250], Loss: 0.1334\n",
            "Epoch [16/100], Step [600/1250], Loss: 0.1329\n",
            "Epoch [16/100], Step [700/1250], Loss: 0.1354\n",
            "Epoch [16/100], Step [800/1250], Loss: 0.1301\n",
            "Epoch [16/100], Step [900/1250], Loss: 0.1329\n",
            "Epoch [16/100], Step [1000/1250], Loss: 0.1354\n",
            "Epoch [16/100], Step [1100/1250], Loss: 0.1331\n",
            "Epoch [16/100], Step [1200/1250], Loss: 0.1358\n",
            "Epoch [17/100], Step [100/1250], Loss: 0.1316\n",
            "Epoch [17/100], Step [200/1250], Loss: 0.1326\n",
            "Epoch [17/100], Step [300/1250], Loss: 0.1309\n",
            "Epoch [17/100], Step [400/1250], Loss: 0.1326\n",
            "Epoch [17/100], Step [500/1250], Loss: 0.1316\n",
            "Epoch [17/100], Step [600/1250], Loss: 0.1311\n",
            "Epoch [17/100], Step [700/1250], Loss: 0.1313\n",
            "Epoch [17/100], Step [800/1250], Loss: 0.1310\n",
            "Epoch [17/100], Step [900/1250], Loss: 0.1314\n",
            "Epoch [17/100], Step [1000/1250], Loss: 0.1306\n",
            "Epoch [17/100], Step [1100/1250], Loss: 0.1335\n",
            "Epoch [17/100], Step [1200/1250], Loss: 0.1332\n",
            "Epoch [18/100], Step [100/1250], Loss: 0.1327\n",
            "Epoch [18/100], Step [200/1250], Loss: 0.1319\n",
            "Epoch [18/100], Step [300/1250], Loss: 0.1295\n",
            "Epoch [18/100], Step [400/1250], Loss: 0.1298\n",
            "Epoch [18/100], Step [500/1250], Loss: 0.1359\n",
            "Epoch [18/100], Step [600/1250], Loss: 0.1311\n",
            "Epoch [18/100], Step [700/1250], Loss: 0.1338\n",
            "Epoch [18/100], Step [800/1250], Loss: 0.1328\n",
            "Epoch [18/100], Step [900/1250], Loss: 0.1299\n",
            "Epoch [18/100], Step [1000/1250], Loss: 0.1306\n",
            "Epoch [18/100], Step [1100/1250], Loss: 0.1323\n",
            "Epoch [18/100], Step [1200/1250], Loss: 0.1317\n",
            "Epoch [19/100], Step [100/1250], Loss: 0.1260\n",
            "Epoch [19/100], Step [200/1250], Loss: 0.1336\n",
            "Epoch [19/100], Step [300/1250], Loss: 0.1256\n",
            "Epoch [19/100], Step [400/1250], Loss: 0.1265\n",
            "Epoch [19/100], Step [500/1250], Loss: 0.1306\n",
            "Epoch [19/100], Step [600/1250], Loss: 0.1313\n",
            "Epoch [19/100], Step [700/1250], Loss: 0.1296\n",
            "Epoch [19/100], Step [800/1250], Loss: 0.1273\n",
            "Epoch [19/100], Step [900/1250], Loss: 0.1305\n",
            "Epoch [19/100], Step [1000/1250], Loss: 0.1337\n",
            "Epoch [19/100], Step [1100/1250], Loss: 0.1273\n",
            "Epoch [19/100], Step [1200/1250], Loss: 0.1286\n",
            "Epoch [20/100], Step [100/1250], Loss: 0.1302\n",
            "Epoch [20/100], Step [200/1250], Loss: 0.1286\n",
            "Epoch [20/100], Step [300/1250], Loss: 0.1294\n",
            "Epoch [20/100], Step [400/1250], Loss: 0.1292\n",
            "Epoch [20/100], Step [500/1250], Loss: 0.1265\n",
            "Epoch [20/100], Step [600/1250], Loss: 0.1289\n",
            "Epoch [20/100], Step [700/1250], Loss: 0.1289\n",
            "Epoch [20/100], Step [800/1250], Loss: 0.1323\n",
            "Epoch [20/100], Step [900/1250], Loss: 0.1257\n",
            "Epoch [20/100], Step [1000/1250], Loss: 0.1276\n",
            "Epoch [20/100], Step [1100/1250], Loss: 0.1301\n",
            "Epoch [20/100], Step [1200/1250], Loss: 0.1296\n",
            "Epoch [21/100], Step [100/1250], Loss: 0.1265\n",
            "Epoch [21/100], Step [200/1250], Loss: 0.1318\n",
            "Epoch [21/100], Step [300/1250], Loss: 0.1300\n",
            "Epoch [21/100], Step [400/1250], Loss: 0.1241\n",
            "Epoch [21/100], Step [500/1250], Loss: 0.1305\n",
            "Epoch [21/100], Step [600/1250], Loss: 0.1292\n",
            "Epoch [21/100], Step [700/1250], Loss: 0.1286\n",
            "Epoch [21/100], Step [800/1250], Loss: 0.1283\n",
            "Epoch [21/100], Step [900/1250], Loss: 0.1281\n",
            "Epoch [21/100], Step [1000/1250], Loss: 0.1333\n",
            "Epoch [21/100], Step [1100/1250], Loss: 0.1255\n",
            "Epoch [21/100], Step [1200/1250], Loss: 0.1270\n",
            "Epoch [22/100], Step [100/1250], Loss: 0.1261\n",
            "Epoch [22/100], Step [200/1250], Loss: 0.1263\n",
            "Epoch [22/100], Step [300/1250], Loss: 0.1287\n",
            "Epoch [22/100], Step [400/1250], Loss: 0.1285\n",
            "Epoch [22/100], Step [500/1250], Loss: 0.1312\n",
            "Epoch [22/100], Step [600/1250], Loss: 0.1300\n",
            "Epoch [22/100], Step [700/1250], Loss: 0.1312\n",
            "Epoch [22/100], Step [800/1250], Loss: 0.1257\n",
            "Epoch [22/100], Step [900/1250], Loss: 0.1293\n",
            "Epoch [22/100], Step [1000/1250], Loss: 0.1267\n",
            "Epoch [22/100], Step [1100/1250], Loss: 0.1264\n",
            "Epoch [22/100], Step [1200/1250], Loss: 0.1238\n",
            "Epoch [23/100], Step [100/1250], Loss: 0.1269\n",
            "Epoch [23/100], Step [200/1250], Loss: 0.1247\n",
            "Epoch [23/100], Step [300/1250], Loss: 0.1244\n",
            "Epoch [23/100], Step [400/1250], Loss: 0.1261\n",
            "Epoch [23/100], Step [500/1250], Loss: 0.1264\n",
            "Epoch [23/100], Step [600/1250], Loss: 0.1324\n",
            "Epoch [23/100], Step [700/1250], Loss: 0.1250\n",
            "Epoch [23/100], Step [800/1250], Loss: 0.1262\n",
            "Epoch [23/100], Step [900/1250], Loss: 0.1247\n",
            "Epoch [23/100], Step [1000/1250], Loss: 0.1287\n",
            "Epoch [23/100], Step [1100/1250], Loss: 0.1243\n",
            "Epoch [23/100], Step [1200/1250], Loss: 0.1268\n",
            "Epoch [24/100], Step [100/1250], Loss: 0.1260\n",
            "Epoch [24/100], Step [200/1250], Loss: 0.1277\n",
            "Epoch [24/100], Step [300/1250], Loss: 0.1194\n",
            "Epoch [24/100], Step [400/1250], Loss: 0.1265\n",
            "Epoch [24/100], Step [500/1250], Loss: 0.1265\n",
            "Epoch [24/100], Step [600/1250], Loss: 0.1199\n",
            "Epoch [24/100], Step [700/1250], Loss: 0.1280\n",
            "Epoch [24/100], Step [800/1250], Loss: 0.1271\n",
            "Epoch [24/100], Step [900/1250], Loss: 0.1280\n",
            "Epoch [24/100], Step [1000/1250], Loss: 0.1254\n",
            "Epoch [24/100], Step [1100/1250], Loss: 0.1272\n",
            "Epoch [24/100], Step [1200/1250], Loss: 0.1215\n",
            "Epoch [25/100], Step [100/1250], Loss: 0.1247\n",
            "Epoch [25/100], Step [200/1250], Loss: 0.1253\n",
            "Epoch [25/100], Step [300/1250], Loss: 0.1243\n",
            "Epoch [25/100], Step [400/1250], Loss: 0.1258\n",
            "Epoch [25/100], Step [500/1250], Loss: 0.1214\n",
            "Epoch [25/100], Step [600/1250], Loss: 0.1230\n",
            "Epoch [25/100], Step [700/1250], Loss: 0.1268\n",
            "Epoch [25/100], Step [800/1250], Loss: 0.1272\n",
            "Epoch [25/100], Step [900/1250], Loss: 0.1257\n",
            "Epoch [25/100], Step [1000/1250], Loss: 0.1251\n",
            "Epoch [25/100], Step [1100/1250], Loss: 0.1239\n",
            "Epoch [25/100], Step [1200/1250], Loss: 0.1250\n",
            "Epoch [26/100], Step [100/1250], Loss: 0.1258\n",
            "Epoch [26/100], Step [200/1250], Loss: 0.1287\n",
            "Epoch [26/100], Step [300/1250], Loss: 0.1241\n",
            "Epoch [26/100], Step [400/1250], Loss: 0.1260\n",
            "Epoch [26/100], Step [500/1250], Loss: 0.1201\n",
            "Epoch [26/100], Step [600/1250], Loss: 0.1256\n",
            "Epoch [26/100], Step [700/1250], Loss: 0.1263\n",
            "Epoch [26/100], Step [800/1250], Loss: 0.1306\n",
            "Epoch [26/100], Step [900/1250], Loss: 0.1199\n",
            "Epoch [26/100], Step [1000/1250], Loss: 0.1234\n",
            "Epoch [26/100], Step [1100/1250], Loss: 0.1221\n",
            "Epoch [26/100], Step [1200/1250], Loss: 0.1203\n",
            "Epoch [27/100], Step [100/1250], Loss: 0.1216\n",
            "Epoch [27/100], Step [200/1250], Loss: 0.1249\n",
            "Epoch [27/100], Step [300/1250], Loss: 0.1285\n",
            "Epoch [27/100], Step [400/1250], Loss: 0.1258\n",
            "Epoch [27/100], Step [500/1250], Loss: 0.1247\n",
            "Epoch [27/100], Step [600/1250], Loss: 0.1254\n",
            "Epoch [27/100], Step [700/1250], Loss: 0.1218\n",
            "Epoch [27/100], Step [800/1250], Loss: 0.1264\n",
            "Epoch [27/100], Step [900/1250], Loss: 0.1193\n",
            "Epoch [27/100], Step [1000/1250], Loss: 0.1212\n",
            "Epoch [27/100], Step [1100/1250], Loss: 0.1250\n",
            "Epoch [27/100], Step [1200/1250], Loss: 0.1219\n",
            "Epoch [28/100], Step [100/1250], Loss: 0.1260\n",
            "Epoch [28/100], Step [200/1250], Loss: 0.1224\n",
            "Epoch [28/100], Step [300/1250], Loss: 0.1183\n",
            "Epoch [28/100], Step [400/1250], Loss: 0.1232\n",
            "Epoch [28/100], Step [500/1250], Loss: 0.1233\n",
            "Epoch [28/100], Step [600/1250], Loss: 0.1275\n",
            "Epoch [28/100], Step [700/1250], Loss: 0.1249\n",
            "Epoch [28/100], Step [800/1250], Loss: 0.1237\n",
            "Epoch [28/100], Step [900/1250], Loss: 0.1239\n",
            "Epoch [28/100], Step [1000/1250], Loss: 0.1245\n",
            "Epoch [28/100], Step [1100/1250], Loss: 0.1217\n",
            "Epoch [28/100], Step [1200/1250], Loss: 0.1240\n",
            "Epoch [29/100], Step [100/1250], Loss: 0.1215\n",
            "Epoch [29/100], Step [200/1250], Loss: 0.1205\n",
            "Epoch [29/100], Step [300/1250], Loss: 0.1230\n",
            "Epoch [29/100], Step [400/1250], Loss: 0.1198\n",
            "Epoch [29/100], Step [500/1250], Loss: 0.1249\n",
            "Epoch [29/100], Step [600/1250], Loss: 0.1249\n"
          ]
        }
      ]
    }
  ]
}